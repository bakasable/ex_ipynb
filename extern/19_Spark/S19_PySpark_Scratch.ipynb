{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- datazone's [introduction ot spark session](https://dzone.com/articles/introduction-to-spark-session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `SparkContext` represents the\n",
    "connection to a Spark cluster, and can be used to create `RDD` and\n",
    "broadcast variables on that cluster. Before Spark2, the entry point to Spark Core was `SparkContext`. `streamingContext` are used for streaming, `sqlContext` for SQL (deprecated) and `hiveContext` for Hive (deprecated) functionnalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated:\n",
    "# from pyspark import (SparkContext, HiveContext)\n",
    "# from pyspark.sql import SQLContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Spark2, a new entry point build for DataSet and DataFrame is available: `SparkSession` replacing `SQLContext` and `hiveContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "[Usage examples](https://www.programcreek.com/python/example/100654/pyspark.sql.SparkSession) at programcreek.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties [configuration](https://spark.apache.org/docs/latest/configuration.html#available-properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"BIOS-821\")\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        property\n",
       "\u001b[0;31mString form:\u001b[0m <property object at 0x7f3cc611e188>\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Runtime configuration interface for Spark.\n",
       "\n",
       "This is the interface through which the user can get and set all Spark and Hadoop\n",
       "configurations that are relevant to Spark SQL. When getting the value of a config,\n",
       "this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
       "\n",
       ".. versionadded:: 2.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.conf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.executor.cores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession` also includes a catalog method that contains methods to work with the metastore (i.e. data catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparkHome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpyFiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mserializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjsc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprofiler_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'pyspark.profiler.BasicProfiler'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Main entry point for Spark functionality. A SparkContext represents the\n",
       "connection to a Spark cluster, and can be used to create L{RDD} and\n",
       "broadcast variables on that cluster.\n",
       "\n",
       ".. note:: :class:`SparkContext` instance is not supported to share across multiple\n",
       "    processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
       "    Use threads instead for concurrent processing purpose.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new SparkContext. At least the master and app name should be set,\n",
       "either through the named parameters here or through C{conf}.\n",
       "\n",
       ":param master: Cluster URL to connect to\n",
       "       (e.g. mesos://host:port, spark://host:port, local[4]).\n",
       ":param appName: A name for your job, to display on the cluster web UI.\n",
       ":param sparkHome: Location where Spark is installed on cluster nodes.\n",
       ":param pyFiles: Collection of .zip or .py files to send to the cluster\n",
       "       and add to PYTHONPATH.  These can be paths on the local file\n",
       "       system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
       ":param environment: A dictionary of environment variables to set on\n",
       "       worker nodes.\n",
       ":param batchSize: The number of Python objects represented as a single\n",
       "       Java object. Set 1 to disable batching, 0 to automatically choose\n",
       "       the batch size based on object sizes, or -1 to use an unlimited\n",
       "       batch size\n",
       ":param serializer: The serializer for RDDs.\n",
       ":param conf: A L{SparkConf} object setting Spark properties.\n",
       ":param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
       "       will be instantiated.\n",
       ":param jsc: The JavaSparkContext instance (optional).\n",
       ":param profiler_cls: A class of custom Profiler used to do profiling\n",
       "       (default is pyspark.profiler.BasicProfiler).\n",
       "\n",
       "\n",
       ">>> from pyspark.context import SparkContext\n",
       ">>> sc = SparkContext('local', 'test')\n",
       "\n",
       ">>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "ValueError:...\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/spark/python/pyspark/context.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyspark.SparkContext?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the Underlying SparksContext to `SparkContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "# master = 'local[*]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test.csv\n"
     ]
    }
   ],
   "source": [
    "%%file data/test.csv\n",
    "number,letter\n",
    "0,a\n",
    "1,c\n",
    "2,b\n",
    "3,a\n",
    "4,b\n",
    "5,c\n",
    "6,a\n",
    "7,a\n",
    "8,a\n",
    "9,b\n",
    "10,b\n",
    "11,c\n",
    "12,c\n",
    "13,b\n",
    "14,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    option('inferSchema', 'true').\n",
    "    load('data/test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     0|     a|\n",
      "|     1|     c|\n",
      "|     2|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: integer (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit schema\n",
    "\n",
    "For production use, you should provide an explicit schema to reduce risk of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"number\", T.DoubleType()),\n",
    "    T.StructField(\"letter\", T.StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    schema(schema).\n",
    "    load('data/test.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   1.0|     c|\n",
      "|   2.0|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: double (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[number: double, letter: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|   0.0|\n",
      "|   1.0|\n",
      "|   2.0|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('number').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koalas\n",
    "\n",
    "See [databick video](https://medium.com/@kyleake/bye-pandas-meet-koalas-pandas-apis-on-apache-spark-ep-4-aedcd363cf4e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import koalas as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfk = kl.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>letter</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        number\n",
       "letter        \n",
       "a            5\n",
       "b            6\n",
       "c            4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfk.groupby('letter').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "|  2.0|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('number').alias('index')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr('number as x')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  x|letter|\n",
      "+---+------+\n",
      "|0.0|     a|\n",
      "|1.0|     c|\n",
      "|2.0|     b|\n",
      "+---+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('number', 'x').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   2.0|     b|\n",
      "|   4.0|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('number % 2 == 0').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|  14.0|     b|\n",
      "|  13.0|     b|\n",
      "|  12.0|     c|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.number.desc()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   1.0|     c|\n",
      "|   5.0|     c|\n",
      "|  11.0|     c|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.letter.desc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|0.0|\n",
      "|2.0|\n",
      "|4.0|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('number*2 as x').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---+\n",
      "|number|letter|  x|\n",
      "+------+------+---+\n",
      "|   0.0|     a|0.0|\n",
      "|   1.0|     c|2.0|\n",
      "|   2.0|     b|4.0|\n",
      "+------+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('x', expr('number*2')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+\n",
      "|min(number)|max(number)|min(letter)|max(letter)|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "|        0.0|       14.0|          a|          c|\n",
      "+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.min('number'), F.max('number'), F.min('letter'), F.max('letter')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-------------------+\n",
      "|letter|      avg(number)|stddev_samp(number)|\n",
      "+------+-----------------+-------------------+\n",
      "|     c|             7.25|  5.188127472091127|\n",
      "|     b|8.666666666666666|  4.802776974487434|\n",
      "|     a|              4.8|  3.271085446759225|\n",
      "+------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').agg(F.mean('number'), F.stddev_samp('number')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = (\n",
    "    Window.partitionBy('letter').\n",
    "    orderBy(F.desc('number')).\n",
    "    rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|letter|sum(number)|\n",
      "+------+-----------+\n",
      "|     c|       29.0|\n",
      "|     b|       52.0|\n",
      "|     a|       24.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').agg(F.sum('number')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   1.0|     c|\n",
      "|   2.0|     b|\n",
      "|   3.0|     a|\n",
      "|   4.0|     b|\n",
      "|   5.0|     c|\n",
      "|   6.0|     a|\n",
      "|   7.0|     a|\n",
      "|   8.0|     a|\n",
      "|   9.0|     b|\n",
      "|  10.0|     b|\n",
      "|  11.0|     c|\n",
      "|  12.0|     c|\n",
      "|  13.0|     b|\n",
      "|  14.0|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|letter|rank|\n",
      "+------+----+\n",
      "|     c|12.0|\n",
      "|     c|23.0|\n",
      "|     c|28.0|\n",
      "|     c|29.0|\n",
      "|     b|14.0|\n",
      "|     b|27.0|\n",
      "|     b|37.0|\n",
      "|     b|46.0|\n",
      "|     b|50.0|\n",
      "|     b|52.0|\n",
      "|     a| 8.0|\n",
      "|     a|15.0|\n",
      "|     a|21.0|\n",
      "|     a|24.0|\n",
      "|     a|24.0|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('letter', F.sum('number').over(ws).alias('rank')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|   0.0|     a|\n",
      "|   1.0|     c|\n",
      "|   2.0|     b|\n",
      "+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT * FROM df_table''').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------------+\n",
      "|letter|             mean|               sd|\n",
      "+------+-----------------+-----------------+\n",
      "|     c|             12.0|              NaN|\n",
      "|     b|              7.5|5.507570547286102|\n",
      "|     a|4.666666666666667|4.163331998932265|\n",
      "+------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT letter, mean(number) AS mean, stddev_samp(number) AS sd from df_table\n",
    "WHERE number % 2 = 0\n",
    "GROUP BY letter\n",
    "ORDER BY letter DESC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String operatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, lower, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spark.createDataFrame(pd.DataFrame(dict(sents=('Thing 1 and Thing 2', 'The Quick Brown Fox'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              sents|\n",
      "+-------------------+\n",
      "|Thing 1 and Thing 2|\n",
      "|The Quick Brown Fox|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = (\n",
    "    s.select(explode(split(lower(expr('sents')), ' '))).\n",
    "    sort('col')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  col|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|  and|\n",
      "|brown|\n",
      "|  fox|\n",
      "|quick|\n",
      "|  the|\n",
      "|thing|\n",
      "|thing|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  col|count|\n",
      "+-----+-----+\n",
      "|    1|    1|\n",
      "|    2|    1|\n",
      "|  and|    1|\n",
      "|brown|    1|\n",
      "|  fox|    1|\n",
      "|quick|    1|\n",
      "|  the|    1|\n",
      "|thing|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1.groupby('col').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.createOrReplaceTempView('s_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|regexp_replace(sents, T.*?g, FOO)|\n",
      "+---------------------------------+\n",
      "|                  FOO 1 and FOO 2|\n",
      "|              The Quick Brown Fox|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT regexp_replace(sents, 'T.*?g', 'FOO')\n",
    "FROM s_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log1p, randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------+\n",
      "|number|     LOG1P(number)|letter|\n",
      "+------+------------------+------+\n",
      "|   0.0|               0.0|     a|\n",
      "|   1.0|0.6931471805599453|     c|\n",
      "|   2.0|1.0986122886681096|     b|\n",
      "+------+------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('number', 'log1p(number)', 'letter').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08685576584741893"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr('number', 'randn() as random').stat.corr('number', 'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = (\n",
    "    spark.range(3).\n",
    "    withColumn('today', F.current_date()).\n",
    "    withColumn('tomorrow', F.date_add('today', 1)).\n",
    "    withColumn('time', F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+\n",
      "| id|     today|  tomorrow|                time|\n",
      "+---+----------+----------+--------------------+\n",
      "|  0|2018-11-19|2018-11-20|2018-11-19 16:19:...|\n",
      "|  1|2018-11-19|2018-11-20|2018-11-19 16:19:...|\n",
      "|  2|2018-11-19|2018-11-20|2018-11-19 16:19:...|\n",
      "+---+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- tomorrow: date (nullable = false)\n",
      " |-- time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test_null.csv\n"
     ]
    }
   ],
   "source": [
    "%%file data/test_null.csv\n",
    "number,letter\n",
    "0,a\n",
    "1,\n",
    "2,b\n",
    "3,a\n",
    "4,b\n",
    "5,\n",
    "6,a\n",
    "7,a\n",
    "8,\n",
    "9,b\n",
    "10,b\n",
    "11,c\n",
    "12,\n",
    "13,b\n",
    "14,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = (\n",
    "    spark.read.\n",
    "    format('csv').\n",
    "    option('header', 'true').\n",
    "    option('inferSchema', 'true').\n",
    "    load('data/test_null.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: integer (nullable = true)\n",
      " |-- letter: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     0|     a|\n",
      "|     1|  null|\n",
      "|     2|     b|\n",
      "|     3|     a|\n",
      "|     4|     b|\n",
      "|     5|  null|\n",
      "|     6|     a|\n",
      "|     7|     a|\n",
      "|     8|  null|\n",
      "|     9|     b|\n",
      "|    10|     b|\n",
      "|    11|     c|\n",
      "|    12|  null|\n",
      "|    13|     b|\n",
      "|    14|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|number|letter|\n",
      "+------+------+\n",
      "|     0|     a|\n",
      "|     2|     b|\n",
      "|     3|     a|\n",
      "|     4|     b|\n",
      "|     6|     a|\n",
      "|     7|     a|\n",
      "|     9|     b|\n",
      "|    10|     b|\n",
      "|    11|     c|\n",
      "|    13|     b|\n",
      "|    14|     b|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|number| letter|\n",
      "+------+-------+\n",
      "|     0|      a|\n",
      "|     1|Missing|\n",
      "|     2|      b|\n",
      "|     3|      a|\n",
      "|     4|      b|\n",
      "|     5|Missing|\n",
      "|     6|      a|\n",
      "|     7|      a|\n",
      "|     8|Missing|\n",
      "|     9|      b|\n",
      "|    10|      b|\n",
      "|    11|      c|\n",
      "|    12|Missing|\n",
      "|    13|      b|\n",
      "|    14|      b|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dn.na.fill('Missing').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF\n",
    "\n",
    "To avoid degrading performance, avoid using UDF if you can use the functions in `pyspark.sql.functions`. If you must use UDFs, prefer `pandas_udf` to `udf` where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('double')\n",
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|number|square(number)|\n",
      "+------+--------------+\n",
      "|   0.0|           0.0|\n",
      "|   1.0|           1.0|\n",
      "|   2.0|           4.0|\n",
      "+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('number', square('number')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double')\n",
    "def scale(x):\n",
    "    return (x - x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|number|      scale(number)|\n",
      "+------+-------------------+\n",
      "|   0.0|-1.5652475842498528|\n",
      "|   1.0|-1.3416407864998738|\n",
      "|   2.0| -1.118033988749895|\n",
      "+------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('number', scale('number')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double', F.PandasUDFType.GROUPED_AGG)\n",
    "def gmean(x):\n",
    "    return x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|letter|    gmean(number)|\n",
      "+------+-----------------+\n",
      "|     c|             7.25|\n",
      "|     b|8.666666666666666|\n",
      "|     a|              4.8|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').agg(gmean('number')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(df.schema, F.PandasUDFType.GROUPED_MAP)\n",
    "def gscale(pdf):\n",
    "    return pdf.assign(number = (pdf.number - pdf.number.mean())/pdf.number.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              number|letter|\n",
      "+--------------------+------+\n",
      "| -1.2046735616310666|     c|\n",
      "|-0.43368248218718397|     c|\n",
      "|    0.72280413697864|     c|\n",
      "|  0.9155519068396106|     c|\n",
      "| -1.3880858307767148|     b|\n",
      "| -0.9716600815437003|     b|\n",
      "| 0.06940429153883587|     b|\n",
      "|  0.2776171661553431|     b|\n",
      "|  0.9022557900048648|     b|\n",
      "|  1.1104686646213722|     b|\n",
      "|  -1.467402817237783|     a|\n",
      "| -0.5502760564641687|     a|\n",
      "| 0.36685070430944583|     a|\n",
      "|  0.6725596245673173|     a|\n",
      "|  0.9782685448251889|     a|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('letter').apply(gscale).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'ann ann bob bob chcuk'.split()\n",
    "courses = '821 823 821 824 823'.split()\n",
    "pdf1 = pd.DataFrame(dict(name=names, course=courses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ann</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bob</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chcuk</td>\n",
       "      <td>823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name course\n",
       "0    ann    821\n",
       "1    ann    823\n",
       "2    bob    821\n",
       "3    bob    824\n",
       "4  chcuk    823"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_id = '821 822 823 824 825'.split()\n",
    "course_names = 'Unix Python R Spark GLM'.split()\n",
    "pdf2 = pd.DataFrame(dict(course_id=course_id, name=course_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>821</td>\n",
       "      <td>Unix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>822</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>823</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>824</td>\n",
       "      <td>Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825</td>\n",
       "      <td>GLM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  course_id    name\n",
       "0       821    Unix\n",
       "1       822  Python\n",
       "2       823       R\n",
       "3       824   Spark\n",
       "4       825     GLM"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(pdf1)\n",
    "df2 = spark.createDataFrame(pdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+-----+\n",
      "| name|course|course_id| name|\n",
      "+-----+------+---------+-----+\n",
      "|  ann|   823|      823|    R|\n",
      "|chcuk|   823|      823|    R|\n",
      "|  bob|   824|      824|Spark|\n",
      "|  ann|   821|      821| Unix|\n",
      "|  bob|   821|      821| Unix|\n",
      "+-----+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.course == df2.course_id, how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+\n",
      "| name|course|course_id|  name|\n",
      "+-----+------+---------+------+\n",
      "|  ann|   823|      823|     R|\n",
      "|chcuk|   823|      823|     R|\n",
      "|  bob|   824|      824| Spark|\n",
      "| null|  null|      825|   GLM|\n",
      "| null|  null|      822|Python|\n",
      "|  ann|   821|      821|  Unix|\n",
      "|  bob|   821|      821|  Unix|\n",
      "+-----+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.course == df2.course_id, how='right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
